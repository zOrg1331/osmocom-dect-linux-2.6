/*
 * DECT transceiver and transceiver group functions
 *
 * Copyright (c) 2009 Patrick McHardy <kaber@trash.net>
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 */

//#define DEBUG
#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/init.h>
#include <linux/mutex.h>
#include <linux/list.h>
#include <linux/notifier.h>
#include <net/dect/dect.h>
#include <net/dect/mac_csf.h>
#include <net/dect/transceiver.h>

static RAW_NOTIFIER_HEAD(dect_transceiver_chain);
LIST_HEAD(dect_transceiver_list);

#define trx_debug(trx, fmt, args...) \
	pr_debug("%s: " fmt, trx->name, ## args)

static const struct dect_band *dect_band[DECT_BAND_NUM];
static const u8 dect_pkt_size[] = {
	[DECT_PACKET_P00] = DECT_P00_SIZE,
	[DECT_PACKET_P08] = DECT_P08_SIZE,
	[DECT_PACKET_P32] = DECT_P32_SIZE,
	[DECT_PACKET_P80] = DECT_P80_SIZE,
};

/**
 * dect_transceiver_alloc_skb - allocate a transceiver RX skb
 *
 * @trx:	transceiver
 * @slot:	slot number
 *
 * Allocate a skb according to the receiving channel characteristics.
 */
struct sk_buff *dect_transceiver_alloc_skb(struct dect_transceiver *trx, u8 slot)
{
	const struct dect_transceiver_slot *ts = &trx->slots[slot];
	struct sk_buff *skb;

	skb = alloc_skb(dect_pkt_size[ts->chd.pkt], GFP_ATOMIC);
	if (skb == NULL)
		return NULL;
	DECT_TRX_CB(skb)->trx = trx;
	DECT_TRX_CB(skb)->slot = ts->chd.slot;
	/* Reserve room for preamble and set up adjacent packet data pointer */
	skb_reserve(skb, DECT_PREAMBLE_SIZE);
	skb_put(skb, dect_pkt_size[ts->chd.pkt] - DECT_PREAMBLE_SIZE);
	return skb;
}
EXPORT_SYMBOL_GPL(dect_transceiver_alloc_skb);

/* Transceiver virtual clock maintenance:
 *
 * The transceiver layer processes the received frames from some slots in
 * the past while the transceiver is transceiving on the following slots.
 * Additionally the transceiver needs to be maintained for the upcoming
 * slots. Therefore there are three different reference frames of time:
 *
 * 		[  RX  ][  TRX ][  TX  ]
 * slot_0		   ->			slot_23
 *
 * - Real time, which is only known to the transceiver
 *
 * - RX time, which is a virtual clock following real time by at least one
 *   slot and advancing as received slots are processed. A transceiver must
 *   generate enough events so that the RX time never lags behind the TRX
 *   time for more than one TDMA half frame.
 *
 * - TX time, which is a virtual clock leading RX time by a usually constant
 *   amount of slots large enough so that packets queued to the transceiver
 *   will reach the transceiver in time to be sent. It always leads real time,
 *   but must never have a distance greater than one TDMA half frame from RX
 *   time, resulting in a maximal distance of 11 to real time.
 *
 * Transceivers periodically notify the transceiver layer of an elapsed amount
 * of time and the frames that were received during that period. The events
 * batches generated by multiple transceivers contained in a group are merged
 * and processed as a single chronological event stream.
 *
 * The following steps are performed during processing (for every slot):
 *
 * - Received packets are passed to the MAC layer
 * - A RX tick is generated, ending the last RX slot
 * - A TX tick with some offset in the future is generated, beginning the
 *   next TX slot
 *
 * The RX and TX ticks are used by the MAC layer to maintain two timer bases
 * for performing maintenance operations after a slot was received or before
 * a slot will be transmitted.
 */

/**
 * dect_transceiver_queue_event - queue a transceiver event for BH processing 
 *
 * @trx:	DECT transceiver
 * @event:	Transceiver event
 */
void dect_transceiver_queue_event(struct dect_transceiver *trx,
				  struct dect_transceiver_event *event)
{
	struct dect_transceiver_group *grp = &trx->cell->trg;

	spin_lock(&grp->lock);
	list_add_tail(&event->list, &grp->events);
	spin_unlock(&grp->lock);

	tasklet_hi_schedule(&grp->tasklet);
}
EXPORT_SYMBOL_GPL(dect_transceiver_queue_event);

static struct dect_transceiver_event *
dect_dequeue_event(struct dect_transceiver_group *grp)
{
	struct dect_transceiver_event *event;
	unsigned long flags;

	event = NULL;
	spin_lock_irqsave(&grp->lock, flags);
	if (!list_empty(&grp->events)) {
		event = list_first_entry(&grp->events,
					 struct dect_transceiver_event,
					 list);
		list_del(&event->list);
	}
	spin_unlock_irqrestore(&grp->lock, flags);
	return event;
}

static void dect_tg_merge_events(struct dect_transceiver_group *grp,
				 struct dect_transceiver *trx,
				 struct dect_transceiver_event *event)
{
	struct sk_buff *skb;
	u8 slot, idx, i;

	/* Transfer the packets to the slot input queues and mark the
	 * slot events.
	 */
	trx_debug(trx, "merge %u events pos %u rssi_mask %x\n",
		  trx->ops->eventrate, event->slotpos, event->rssi_mask);

	for (i = 0; i < trx->ops->eventrate; i++) {
		slot = event->slotpos + i;
		idx = slot % ARRAY_SIZE(grp->slots);

		skb = skb_peek(&event->rx_queue);
		if (skb != NULL && DECT_TRX_CB(skb)->slot == slot) {
			__skb_unlink(skb, &event->rx_queue);
			__skb_queue_tail(&grp->slots[idx].queue, skb);
		} else if (event->rssi_mask & (1 << i))
			grp->slots[idx].rssi[trx->index] = event->rssi[i];

		grp->slots[idx].mask |= 1 << trx->index;
	}
}

static bool seqno_before(u32 seq1, u32 seq2)
{
	return (s32)(seq2 - seq1) > 0;
}

static bool seqno_after(u32 seq1, u32 seq2)
{
	return seqno_before(seq2, seq1);
}

static void dect_tg_process_events(struct dect_transceiver_group *grp)
{
	struct dect_transceiver *trx;
	struct dect_transceiver_slot *ts;
	struct sk_buff *skb;
	u8 idx, d, i;
	u16 late;

	pr_debug("process events slot_low=%u slot_high=%u distance=%u\n",
		 grp->slot_low, grp->slot_high,
		 dect_slot_distance(grp->slot_low, grp->slot_high));

	while (grp->slot_low != grp->slot_high) {
		/*
		 * If more than one half frame is missing, only forward the
		 * clock since the slot positions refer to slots in the
		 * following half frame.
		 */
		d = dect_slot_distance(grp->slot_low, grp->slot_high);
		if (d > ARRAY_SIZE(grp->slots))
			goto tick;

		idx = grp->slot_low % ARRAY_SIZE(grp->slots);

		/* Check for transceivers which are lagging by more than their
		 * event rate window and mark the current window entirely as
		 * lost.
		 */
		late = grp->slots[idx].mask ^ grp->trxmask;
		while (late != 0) {
			trx = grp->trx[ffs(late) - 1];
			late &= ~(1 << trx->index);

			if (!seqno_before(trx->seqno + trx->ops->eventrate,
					  grp->seqno) &&
			    d <= trx->ops->eventrate)
				continue;

			trx_debug(trx, "late for window %u\n", grp->slot_low);
			for (i = 0; i < trx->ops->eventrate; i++)
				grp->slots[(idx + i) % 12].mask |= 1 << trx->index;
			trx->stats.event_late++;
		}

		if (grp->slots[idx].mask != grp->trxmask) {
			pr_debug("slot %u incomplete: mask %x trx %x\n",
				grp->slot_low, grp->slots[idx].mask, grp->trxmask);
			break;
		}

		while ((skb = __skb_dequeue(&grp->slots[idx].queue))) {
			trx = DECT_TRX_CB(skb)->trx;
			ts = &trx->slots[DECT_TRX_CB(skb)->slot];
			dect_mac_rcv(trx, ts, skb);
		}

		for (i = 0; i < ARRAY_SIZE(grp->slots[idx].rssi); i++) {
			if (grp->slots[idx].rssi[i] == 0)
				continue;
			trx = grp->trx[i];
			ts = &trx->slots[grp->slot_low];
			dect_mac_report_rssi(trx, ts, grp->slots[idx].rssi[i]);
			grp->slots[idx].rssi[i] = 0;
		}

		grp->slots[idx].mask = 0;
tick:
		dect_mac_rx_tick(grp, dect_next_slotnum(grp->slot_low));
		dect_mac_tx_tick(grp, dect_slot_add(grp->slot_low,
						    2 * grp->latency));

		grp->slot_low = dect_next_slotnum(grp->slot_low);
	}
}

/*
 * Softirq transceiver group event processing
 */
static void dect_transceiver_tasklet(unsigned long data)
{
	struct dect_transceiver_group *grp = (struct dect_transceiver_group *)data;
	struct dect_transceiver *trx;
	struct dect_transceiver_event *event;
	struct sk_buff *skb;

again:
	event = dect_dequeue_event(grp);
	if (event == NULL)
		return;
	trx = event->trx;

	trx_debug(trx, "event handler: trx: seq %u pos %u grp: seq %u pos %u\n",
		  trx->seqno, event->slotpos, grp->seqno, grp->slot_low);

	/* Before a transceiver is locked, its timing might vary and isn't
	 * synchronized to the remaining group. The MAC layer handles this
	 * manually.
	 */
	if (trx->state != DECT_TRANSCEIVER_LOCKED) {
		skb = __skb_dequeue(&event->rx_queue);
		if (skb != NULL)
			dect_mac_irc_rcv(trx, skb);
		dect_mac_irc_tick(trx);
		goto out;
	}

	/* If a secondary transceiver enters locked state or a tranceiver missed
	 * a previous window, its sequence number is out of sync. Resync it once
	 * it starts reporting events in the current window.
	 *
	 * FIXME: driver should ignore frames with missed interrupts completely
	 * FIXME2: off by one window if receiver is not first
	 */
	if (seqno_before(trx->seqno + trx->ops->eventrate, grp->seqno)) {
		if (event->slotpos != grp->slot_low) {
			trx_debug(trx, "unsynchronized\n");
			__skb_queue_purge(&event->rx_queue);
			goto out;
		}
		trx->seqno = grp->seqno;
		trx_debug(trx, "synchronized to seqno %u\n", trx->seqno);
	}

	/* Merge the events and update the sequence number. The transceiver
	 * with the highest sequence number determines the slot position for
	 * the entire group.
	 */
	dect_tg_merge_events(grp, trx, event);

	trx->seqno += trx->ops->eventrate;
	if (seqno_after(trx->seqno, grp->seqno)) {
		grp->seqno = trx->seqno;
		grp->slot_high =
			dect_slot_add(event->slotpos, trx->ops->eventrate);
	}

	dect_tg_process_events(grp);

out:
	dect_release_transceiver_event(event);
	goto again;
}

int dect_transceiver_group_add(struct dect_transceiver_group *grp,
			       struct dect_transceiver *trx)
{
	u8 index;

	index = ffz(grp->trxmask);
	if (index >= ARRAY_SIZE(grp->trx))
		return -EMFILE;

	trx->index = index;
	grp->trx[index] = trx;
	if (trx->ops->latency > grp->latency)
		grp->latency = trx->ops->latency;

	grp->trxmask |= 1 << index;
	return 0;
}

void dect_transceiver_group_remove(struct dect_transceiver_group *grp,
				   struct dect_transceiver *trx)
{
	grp->trxmask &= ~(1 << trx->index);
	/* Synchronize with interrupt and softirq processing */
	synchronize_rcu();
	grp->trx[trx->index] = NULL;
}

void dect_transceiver_group_init(struct dect_transceiver_group *grp)
{
	unsigned int i;

	spin_lock_init(&grp->lock);
	INIT_LIST_HEAD(&grp->events);
	for (i = 0; i < ARRAY_SIZE(grp->slots); i++)
		__skb_queue_head_init(&grp->slots[i].queue);

	tasklet_init(&grp->tasklet, dect_transceiver_tasklet,
		     (unsigned long)grp);
}

void dect_transceiver_disable(struct dect_transceiver *trx)
{
	trx->ops->disable(trx);
	trx->state = DECT_TRANSCEIVER_STOPPED;
}

void dect_transceiver_enable(struct dect_transceiver *trx)
{
	if (trx->mode == DECT_TRANSCEIVER_MASTER)
		trx->state = DECT_TRANSCEIVER_LOCKED;
	else {
		trx->state = DECT_TRANSCEIVER_UNLOCKED;
		trx->slots[DECT_SCAN_SLOT].state = DECT_SLOT_SCANNING;
	}
	trx->ops->enable(trx);
}

void dect_transceiver_confirm(struct dect_transceiver *trx)
{
	trx_debug(trx, "confirm\n");
	trx->state = DECT_TRANSCEIVER_LOCK_PENDING;
	trx->slots[DECT_SCAN_SLOT].state = DECT_SLOT_RX;
	trx->ops->confirm(trx);
}

void dect_transceiver_unlock(struct dect_transceiver *trx)
{
	trx_debug(trx, "unlock\n");
	trx->ops->unlock(trx);
	trx->slots[DECT_SCAN_SLOT].state = DECT_SLOT_SCANNING;
	trx->state = DECT_TRANSCEIVER_UNLOCKED;
}

int dect_transceiver_set_band(struct dect_transceiver *trx, u8 bandnum)
{
	const struct dect_band *band;

	band = dect_band[bandnum];
	if (band == NULL)
		return -ENOENT;
	trx->carriers = trx->ops->set_band(trx, band);
	trx->band = band;
	return 0;
}

void dect_transceiver_lock(struct dect_transceiver *trx, u8 slot)
{
	trx_debug(trx, "lock to slot %u\n", slot);
	trx->slots[DECT_SCAN_SLOT].state = DECT_SLOT_IDLE;
	trx->state = DECT_TRANSCEIVER_LOCKED;
	trx->ops->lock(trx, slot);
}

bool dect_transceiver_channel_available(const struct dect_transceiver *trx,
				        const struct dect_channel_desc *chd)
{
	if (trx->slots[chd->slot].state == DECT_SLOT_RX ||
	    trx->slots[chd->slot].state == DECT_SLOT_TX)
		return false;

	switch ((int)chd->pkt) {
	case DECT_PACKET_P80:
		if (trx->blind_full_slots & (1 << (chd->slot + 1)))
			return false;
	case DECT_PACKET_P32:
	case DECT_PACKET_P08:
	case DECT_PACKET_P00:
		if (trx->blind_full_slots & (1 << chd->slot))
			return false;
		break;
	}
	return true;
}

static bool dect_tg_update_blind_full_slots(struct dect_transceiver_group *trg)
{
	const struct dect_transceiver *trx;
	u32 blind_full_slots;

	blind_full_slots = (~0U) & DECT_SLOT_MASK;
	dect_foreach_transceiver(trx, trg)
		blind_full_slots &= trx->blind_full_slots;

	if (trg->blind_full_slots != blind_full_slots) {
		trg->blind_full_slots = blind_full_slots;
		return true;
	} else
		return false;
}

/**
 * dect_transceiver_reserve - reserve transceiver resources for a physical channel
 *
 * Reserve the slot positions necessary to estabish the specified physical
 * channel. The chosen transceivers and the global groups blind full slot
 * masks are updated.
 *
 * Returns true when the global visibility state has changed.
 */
bool dect_transceiver_reserve(struct dect_transceiver_group *trg,
			      struct dect_transceiver *trx,
			      const struct dect_channel_desc *chd)
{
	switch ((int)chd->pkt) {
	case DECT_PACKET_P80:
		trx->blind_full_slots |= 1 << (chd->slot + 1);
	case DECT_PACKET_P32:
	case DECT_PACKET_P08:
	case DECT_PACKET_P00:
		trx->blind_full_slots |= 1 << chd->slot;
		break;
	}

	return dect_tg_update_blind_full_slots(trg);
}

/**
 * dect_transceiver_release - release transceiver resources of a phyiscal channel
 *
 * Release the slot positions used by the specified physical channel. The
 * transceiver and the global group blind full slot masks are updated.
 *
 * Returns true when the global visibility state has changed.
 */
bool dect_transceiver_release(struct dect_transceiver_group *trg,
			      struct dect_transceiver *trx,
			      const struct dect_channel_desc *chd)
{
	switch ((int)chd->pkt) {
	case DECT_PACKET_P80:
		trx->blind_full_slots &= ~(1 << (chd->slot + 1));
	case DECT_PACKET_P32:
	case DECT_PACKET_P08:
	case DECT_PACKET_P00:
		trx->blind_full_slots &= ~(1 << chd->slot);
		break;
	}

	return dect_tg_update_blind_full_slots(trg);
}

void dect_register_notifier(struct notifier_block *nb)
{
	dect_lock();
	raw_notifier_chain_register(&dect_transceiver_chain, nb);
	dect_unlock();
}

void dect_unregister_notifier(struct notifier_block *nb)
{
	dect_lock();
	raw_notifier_chain_unregister(&dect_transceiver_chain, nb);
	dect_unlock();
}

static void dect_transceiver_notify(unsigned long val,
				    struct dect_transceiver *trx)
{
	raw_notifier_call_chain(&dect_transceiver_chain, val, trx);
}

struct dect_transceiver *dect_transceiver_alloc(const struct dect_transceiver_ops *ops,
						unsigned int priv)
{
	struct dect_transceiver *trx;
	unsigned int nevents, size, i;

	/* Allocate enough event structures for one TDMA half frame */
	nevents = DECT_HALF_FRAME_SIZE / ops->eventrate;
	size = nevents * sizeof(trx->event[0]) + priv;

	trx = kzalloc(sizeof(*trx) + size, GFP_KERNEL);
	if (trx == NULL)
		return NULL;

	trx->state = DECT_TRANSCEIVER_STOPPED;
	trx->ops = ops;
	trx->blind_full_slots = ~ops->slotmask & DECT_SLOT_MASK;
	for (i = 0; i < DECT_FRAME_SIZE; i++)
		trx->slots[i].chd.slot = i;

	for (i = 0; i < nevents; i++) {
		skb_queue_head_init(&trx->event[i].rx_queue);
		trx->event[i].trx = trx;
	}
	return trx;
}
EXPORT_SYMBOL_GPL(dect_transceiver_alloc);

void dect_transceiver_free(struct dect_transceiver *trx)
{
	kfree(trx);
}
EXPORT_SYMBOL_GPL(dect_transceiver_free);

static int dect_transceiver_alloc_name(struct dect_transceiver *trx)
{
	struct dect_transceiver *t;
	unsigned long *inuse;
	int i;

	inuse = (unsigned long *)get_zeroed_page(GFP_KERNEL);
	if (inuse == NULL)
		return -ENOMEM;

	list_for_each_entry(t, &dect_transceiver_list, list) {
		if (!sscanf(t->name, "trx%d", &i))
			continue;
		if (i > BITS_PER_BYTE * PAGE_SIZE)
			continue;
		set_bit(i, inuse);
	}

	i = find_first_zero_bit(inuse, BITS_PER_BYTE * PAGE_SIZE);
	free_page((unsigned long)inuse);
	if (i == BITS_PER_BYTE * PAGE_SIZE)
		return -ENFILE;

	snprintf(trx->name, sizeof(trx->name), "trx%d", i);
	return 0;
}

int dect_register_transceiver(struct dect_transceiver *trx)
{
	int err;

	dect_lock();
	err = dect_transceiver_alloc_name(trx);
	if (err < 0)
		goto out;

	err = dect_transceiver_set_band(trx, DECT_DEFAULT_BAND);
	if (err < 0)
		goto out;

	list_add_tail(&trx->list, &dect_transceiver_list);
	dect_transceiver_notify(DECT_TRANSCEIVER_REGISTER, trx);
out:
	dect_unlock();

	return err;
}
EXPORT_SYMBOL_GPL(dect_register_transceiver);

void dect_unregister_transceiver(struct dect_transceiver *trx)
{
	dect_lock();
	list_del(&trx->list);
	dect_transceiver_notify(DECT_TRANSCEIVER_UNREGISTER, trx);
	dect_unlock();

	synchronize_rcu();
	trx->ops->destructor(trx);
}
EXPORT_SYMBOL_GPL(dect_unregister_transceiver);

/*
 * RF-bands:
 */

struct dect_band_desc {
	u8	band;
	u8	carriers;
	s16	c1_off;
	u8	c2;
	s16	c2_off;
};

static const struct dect_band_desc dect_band_desc[] __initdata = {
	/* 1880 MHz to 1900 MHz RF band 00000 */
	{ 0, 10, 0, -1,  0, },
	/* 1880 MHz to 1978 MHz and 2010 MHz to 2025 MHz RF band 00001 */
	{ 1, 64, 0, 56, 19, },
	/* 1880 MHz to 1925 MHz and 2010 MHz to 2025 MHz RF band 00010 */
	{ 2, 33, 0, 25, 50, },
	/* 1880 MHz to 1900 MHz, 1915 MHz to 1940 MHz and 2010 MHz to
	 * 2025 MHz RF band 00011 */
	{ 3, 33, 10, 25, 50, },
	/* 1880 MHz to 1900 MHz, 1935 MHz to 1960 MHz and 2010 MHz to
	 * 2025 MHz RF band 00100 */
	{ 4, 33, 22, 25, 50, },
	/* 1880 MHz to 1900 MHZ, 1955 MHz to 1980 MHz and 2010 MHz to
	 * 2025 MHz RF band 00101 */
	{ 5, 33, 34, 25, 50, },
	/* 902 MHz to 928 MHz RF band 01000 */
	{ 8, 24, -576, -1, 0, },
	/* 2400 MHz to 2483,5 MHz RF band 01001 */
	{ 9, 55, 292, -1, 0, },
};

/*
 * A nominal DECT RF carrier is one whose center frequency is generated by
 * the formula:
 *
 * Fg = F0 - g × 1,728 MHz, where g is any integer
 */
static u32 __init dect_calc_frequency(s16 g)
{
	if (g >= 0 && g < 10)
		return DECT_FREQUENCY_F0 - g * DECT_CARRIER_WIDTH;
	else
		return DECT_FREQUENCY_F0 + (g - 9) * DECT_CARRIER_WIDTH;
}

static int __init dect_init_band(const struct dect_band_desc *desc)
{
	struct dect_band *band;
	unsigned int size;
	u8 carrier;

	size = sizeof(*band) + desc->carriers * sizeof(band->frequency[0]);
	band = kmalloc(size, GFP_KERNEL);
	if (band == NULL)
		return -ENOMEM;

	band->band     = desc->band;
	band->carriers = desc->carriers;

	for (carrier = 0; carrier < 10; carrier++)
		band->frequency[carrier] =
			dect_calc_frequency(carrier);

	for (; carrier < min(desc->carriers, desc->c2); carrier++)
		band->frequency[carrier] =
			dect_calc_frequency(carrier + desc->c1_off);

	for (; carrier < desc->carriers; carrier++)
		band->frequency[carrier] =
			dect_calc_frequency(carrier + desc->c2_off);

	printk("RF-band %u:\n", band->band);
	for (carrier = 0; carrier < band->carriers; carrier++) {
		printk("  carrier %u: %u.%.3uMHz\n", carrier,
		       band->frequency[carrier] / 1000,
		       band->frequency[carrier] % 1000);
	}
	printk("\n");

	dect_band[band->band] = band;
	return 0;
}

int __init dect_transceiver_module_init(void)
{
	unsigned int i;
	int err;

	for (i = 0; i < ARRAY_SIZE(dect_band_desc); i++) {
		err = dect_init_band(&dect_band_desc[i]);
		if (err < 0)
			goto err1;
	}
	return 0;

err1:
	dect_transceiver_module_exit();
	return err;
}

void dect_transceiver_module_exit(void)
{
	u8 band;

	for (band = 0; band < ARRAY_SIZE(dect_band); band++)
		kfree(dect_band[band]);
}
